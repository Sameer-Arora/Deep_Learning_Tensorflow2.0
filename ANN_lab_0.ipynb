{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_lab_0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sameer-Arora/Deep_Learning_Tensorflow2.0/blob/master/ANN_lab_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVRK7trbTmX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDQ8nNkaVibq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrE4mqYKVY9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "print(mnist)\n",
        "\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuBHwmUZa827",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "print( x_train.shape , y_train.shape ,x_train[0].shape, y_train[0], x_test[0].shape, y_test[0] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdRZdsujfC_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_size = int(0.2*x_train.shape[0])\n",
        "train_size = x_train.shape[0]-val_size\n",
        "(x_train,x_validate) = tf.split(x_train, [ train_size, val_size ] ,axis=0)\n",
        "(y_train,y_validate) = tf.split(y_train, [ train_size, val_size ] ,axis=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA9rajEhbWU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print( x_train.shape , y_train.shape ,x_train[0].shape, y_train[0], x_validate.shape, y_validate.shape )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGwLcPjoNPJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_data( images , labels ):\n",
        "  \"\"\"\n",
        "  Function to convet the images into a flattened vector and \n",
        "  to one-hot encode the labels,\n",
        "  \n",
        "  Returns:\n",
        "  input: input to the network.\n",
        "  labels: output for the loss. \n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  \n",
        "  return tf.reshape(images,[ images.shape[0] ,-1]) , tf.one_hot(indices= tf.cast(labels,dtype=tf.int32),depth= tf.cast( tf.math.reduce_max(labels)+1,dtype=tf.int32) \n",
        "                                                                ,axis=1,dtype=tf.float64)\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTQWdYzAOvKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train,y_train) = preprocess_data(x_train,y_train)\n",
        "# print( x_train.shape , y_train.shape ,x_train[0], y_train[0], x_validate.shape, y_validate.shape )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhSGi3fk5ImT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class MyCustomLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, num_outputs):\n",
        "    super(MyCustomLayer, self).__init__()\n",
        "    self.num_outputs = num_outputs\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.kernel = self.add_variable(\"W\",\n",
        "                                    shape=[int(input_shape[-1]),\n",
        "                                           self.num_outputs])\n",
        "    self.bias = self.add_variable(\"b\",\n",
        "                                    shape=[ 1, self.num_outputs ] )\n",
        "\n",
        "class LinearLayer(MyCustomLayer):\n",
        "\n",
        "  def __init__(self, num_outputs):\n",
        "    super(LinearLayer, self).__init__(num_outputs)\n",
        "    self.num_outputs = num_outputs\n",
        "\n",
        "  def call(self, input):\n",
        "    return tf.matmul(input, self.kernel)+ self.bias\n",
        "\n",
        "class SoftMaxLayer(MyCustomLayer):\n",
        "\n",
        "  def __init__(self, num_outputs):\n",
        "    super(SoftMaxLayer, self).__init__(num_outputs)\n",
        "    self.num_outputs = num_outputs\n",
        "\n",
        "  def call(self, input):\n",
        "    return softmax(tf.matmul(input, self.kernel)+ self.bias)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7NB5pOH5iUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class CustomModel(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self, inp_dim,hid_dim,out_dim ):\n",
        "    super(CustomModel, self).__init__(name='')\n",
        "\n",
        "    self.linear_1 = LinearLayer(hid_dim)\n",
        "    \n",
        "    self.sftmax_2 = SoftMaxLayer(out_dim)\n",
        "\n",
        "  def call(self, input_tensor, training=False):\n",
        "    \n",
        "    x = self.linear_1(input_tensor)\n",
        "    x = self.sftmax_2(x)\n",
        "\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWTR_CgodVMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nn_model_initialize_params( inp_dim,hid_dim,out_dim ):\n",
        "\n",
        "    \n",
        "#     tf.random.set_random_seed(1)\n",
        "\n",
        "    W1 = tf.Variable(tf.zeros([inp_dim, hid_dim ], tf.float64),name=\"W1\")\n",
        "    b1 = tf.Variable(tf.zeros([ 1,hid_dim ], tf.float64),name=\"b1\")\n",
        "    \n",
        "    W2 = tf.Variable(tf.zeros([ hid_dim,out_dim ], tf.float64),name=\"W2\")\n",
        "    b2 = tf.Variable(tf.zeros([ 1,out_dim ], tf.float64),name=\"b2\")\n",
        "    \n",
        "    parameters = {\n",
        "        \"W1\" : W1 ,\n",
        "        \"b1\" : b1 ,\n",
        "        \"W2\" : W2 ,\n",
        "        \"b2\" : b2 ,\n",
        "                 }\n",
        "  \n",
        "    return parameters\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQhdtSvziqW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(Z):\n",
        "  ## rectify for numerical approximations\n",
        "  \n",
        "  return tf.math.exp(Z) / tf.math.reduce_sum( tf.math.exp(Z) )\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jfj9SVrlNxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "softmax(tf.Variable([1.0,2.0], name=\"i1\",dtype=tf.float32))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO5UAxoeDLLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOHu-3-L56pV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feed_forward_model(X,parameters):\n",
        "\n",
        "  W1 = parameters['W1']\n",
        "  b1 = parameters['b1']\n",
        "  W2 = parameters['W2']\n",
        "  b2 = parameters['b2']\n",
        "  \n",
        "  print(X.shape,W1.shape,b1.shape)\n",
        "  Z1=  tf.matmul(X,W1) + b1  ## first linear layer.\n",
        "  \n",
        "  Z2=  tf.matmul(Z1,W2) + b2  ## second softmax layer.\n",
        "  \n",
        "  A2= softmax(Z2)\n",
        "  \n",
        "  return A2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kepQw5FMjODs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(model, inputs, targets):\n",
        "  preds = model(inputs)\n",
        "  d_loss = - tf.math.reduce_sum( tf.multiply( targets , tf.cast(tf.math.log(preds),tf.float64) ) ) \n",
        "  return d_loss / targets.shape[0]\n",
        "\n",
        "def compute_data_loss(y,y_):\n",
        "  print(y.shape,y_.shape)\n",
        "  ## softmax_cross_entropy_loss as data_loss\n",
        "  d_loss = - tf.math.reduce_sum( tf.multiply( y , tf.cast(tf.math.log(y_),tf.float64) ) ) \n",
        "  return d_loss / y.shape[0]\n",
        "\n",
        "def compute_reg_loss(parameters):\n",
        "  W1 = parameters['W1']\n",
        "  W2 = parameters['W2']\n",
        "  ## L2 regularization_loss\n",
        "  return tf.math.reduce_sum(tf.math.reduce_sum( tf.math.multiply(W1,W1) )) + tf.math.reduce_sum(tf.math.reduce_sum( tf.math.multiply(W2,W2) )) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIEymoN3k6Wq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mini_batch(x_train,y_train,batch_size):\n",
        "  assert x_train.shape[0] == y_train.shape[0]\n",
        "  \n",
        "  for i in range(0,x_train.shape[0],batch_size):\n",
        "    yield x_train[i:i+batch_size-1] ,  y_train[i:i+batch_size-1]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaM4tHItqnDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grad(model, inputs, targets):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss_value = loss(model, inputs, targets)\n",
        "  return loss_value, tape.gradient(loss_value, model.trainable_variables)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ang1B8sql52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "518gMEuPhEB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_model( learning_rate,regularization_param, batch_size ,epochs, x_train,y_train,x_test,y_test ):\n",
        "  \n",
        "  model = CustomModel(784,100,10)\n",
        "  parameters = nn_model_initialize_params( 784,100,10 )\n",
        "  \n",
        "  num_tr_exm = x_train.shape[0]\n",
        "  \n",
        "  costs=[]\n",
        "  seed=0\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    num_minibatches = int(num_tr_exm/ batch_size)\n",
        "    epoch_cost=0 \n",
        "    seed =seed+1   \n",
        "    \n",
        "    x_train_shuffle = tf.random.shuffle(x_train,seed=seed)  \n",
        "    y_train_shuffle = tf.random.shuffle(y_train,seed=seed)  \n",
        "  \n",
        "    minibatches_gen = get_mini_batch(x_train,y_train,batch_size)    \n",
        "    \n",
        "    for _ in range(num_minibatches):\n",
        "      \n",
        "      x_train_batch,y_train_batch = next(minibatches_gen)\n",
        "#       print(\"===============================================\")\n",
        "#       print(model(x_train_batch))\n",
        "      \n",
        "#       y_= model(x_train_batch)\n",
        "#       y_ = feed_forward_model(x_train_batch,parameters)\n",
        "      \n",
        "#       data_loss = compute_data_loss(y_train_batch,y_)\n",
        "#       print(data_loss)\n",
        "      \n",
        "#       reg_loss = compute_reg_loss(parameters)\n",
        "#       print(reg_loss)\n",
        "#       loss =  data_loss +  reg_loss \n",
        "\n",
        "      # function to pass loss as required by optimizers in tf 2.0\n",
        "      def return_loss():\n",
        "        return loss    \n",
        "\n",
        "      optimizer = tf.optimizers.SGD(learning_rate=learning_rate,momentum=0.0)\n",
        "      loss_value, grads = grad(model, x_train_batch, y_train_batch)\n",
        "\n",
        "#       print(\"Step: {}, Initial Loss: {}\".format(optimizer.iterations.numpy(),\n",
        "#                                                 loss_value.numpy()))\n",
        "\n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "#       print(\"Step: {},         Loss: {}\".format(optimizer.iterations.numpy(),\n",
        "#                                                 loss(model,x_train_batch, y_train_batch).numpy()))\n",
        "      \n",
        "#       optimizer.minimize(return_loss,var_list=list(parameters.values()))\n",
        "      \n",
        "      epoch_cost += loss_value.numpy() / num_minibatches\n",
        "\n",
        "#       print([x.name for x in model.trainable_variables])\n",
        "#       print(\"===============================================\")\n",
        "      \n",
        "    if epoch % 10 ==0:\n",
        "      print(\"Cost after epoch\",str(epoch) ,\" is \",str(epoch_cost))\n",
        "      \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6SjA5xfgvZr",
        "colab_type": "code",
        "outputId": "76e52d39-9e1a-4d2c-a689-46096dfa9e08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "learning_rate = 0.01\n",
        "regularization_param= 1\n",
        "batch_size=64\n",
        "epochs=100\n",
        "\n",
        "train_model( learning_rate,regularization_param, batch_size ,epochs, x_train,y_train,x_test,y_test )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after epoch 40  is  4.680719354165296\n",
            "Cost after epoch 50  is  4.6753166315542956\n",
            "Cost after epoch 60  is  4.671351144846155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU_P2lAXAlws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqILFcfz2MJM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L81to_Q12L35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvRUG-Qr2Lrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwtIvMFQ2KNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoTDxOvX2JwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoIg8SVb0cHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--mpP6brzm2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cBdOOqHx3Ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84GDY4SbsA-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}